{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CyberbullyingDetection.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8OtQsX6NJ4m"
      },
      "source": [
        "## environment prep"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OQVf4Q-Bcuxl",
        "outputId": "eef1b1d1-0c0a-43dd-8cc3-22376ef0af48"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 194,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.9.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (5.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: huggingface-hub==0.0.12 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub==0.0.12->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LTd7mnAA0Bvk"
      },
      "source": [
        "import torch\n",
        "# import torch_xla\n",
        "# import torch_xla.core.xla_model as xm\n",
        "import string\n",
        "import pandas as pd\n",
        "import re\n",
        "from torch.utils.data import TensorDataset, random_split, DataLoader\n",
        "from transformers import AutoTokenizer, BertForSequenceClassification, AdamW, BertConfig, get_linear_schedule_with_warmup\n",
        "import numpy as np\n",
        "import random"
      ],
      "execution_count": 195,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yGVzf0agWMN7",
        "outputId": "09fba278-c5e7-4cbd-84e4-0991c86668a1"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 196,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AzcsZ1FsNzFZ"
      },
      "source": [
        "## Data prep"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RJi0seVIXWvW",
        "outputId": "78207d0e-ad2c-4fec-cdd3-c4baff7d9798"
      },
      "source": [
        "text = u'This is a smiley face \\U0001f602'\n",
        "print(text) # with emoji\n",
        "\n",
        "def deEmojify(text):\n",
        "    regrex_pattern = re.compile(pattern = \"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           \"]+\", flags = re.UNICODE)\n",
        "    return regrex_pattern.sub(r'',text)\n",
        "\n",
        "print(deEmojify(text))"
      ],
      "execution_count": 197,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "This is a smiley face ðŸ˜‚\n",
            "This is a smiley face \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KpmiCMDcY0Z2"
      },
      "source": [
        "https://stackoverflow.com/questions/33404752/removing-emojis-from-a-string-in-python"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VotN11WtY61u"
      },
      "source": [
        "def text_preprocess(line):\n",
        "  line = line.translate(str.maketrans('', '', string.punctuation))\n",
        "  line = line.replace('anonymizedaccount', '')\n",
        "  line = line.replace('\\n', '')\n",
        "  line = deEmojify(line)\n",
        "  return line.lower()\n",
        "  "
      ],
      "execution_count": 198,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "itOSo67JTc1m"
      },
      "source": [
        "def create_dataset(txt_path, labels_path):\n",
        "  dict = {}\n",
        "  dict['texts'] = []\n",
        "  dict['labels'] = []\n",
        "  with open(txt_path) as txt_file:\n",
        "    for line in txt_file.readlines():\n",
        "      line = text_preprocess(line)\n",
        "      \n",
        "      dict['texts'].append(line)\n",
        "  with open(labels_path) as labels_files:\n",
        "    for label in labels_files.readlines():\n",
        "    \n",
        "      dict['labels'].append(int(label))\n",
        "  df = pd.DataFrame(dict)\n",
        "  return df\n"
      ],
      "execution_count": 199,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYJd_TdRV7A6"
      },
      "source": [
        "df = create_dataset('/content/drive/MyDrive/CyberbullyingDetection/task1/training_set_clean_only_text.txt', '/content/drive/MyDrive/CyberbullyingDetection/task1/training_set_clean_only_tags.txt')"
      ],
      "execution_count": 202,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "nMZBNMJ9Wxbo",
        "outputId": "48fb6a8f-4c39-4917-c4e7-855d632a5f26"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 203,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>texts</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>dla mnie faworytem do tytuÅ‚u bÄ™dzie cracovia z...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>brawo ty daria kibic ma byÄ‡ na dobre i zÅ‚e</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>super polski premier skÅ‚ada kwiaty na grobac...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>musi innej drogi nie mamy</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>odrzut natychmiastowy kwaÅ›na mina mam problem</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               texts  labels\n",
              "0  dla mnie faworytem do tytuÅ‚u bÄ™dzie cracovia z...       0\n",
              "1         brawo ty daria kibic ma byÄ‡ na dobre i zÅ‚e       0\n",
              "2    super polski premier skÅ‚ada kwiaty na grobac...       0\n",
              "3                          musi innej drogi nie mamy       0\n",
              "4      odrzut natychmiastowy kwaÅ›na mina mam problem       0"
            ]
          },
          "metadata": {},
          "execution_count": 203
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7DKB_yft1BA5"
      },
      "source": [
        "df = df.sample(frac = 1)"
      ],
      "execution_count": 204,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmaqlqyTOEFT"
      },
      "source": [
        "## model bert for sequence classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yiR2Fat9BuW"
      },
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"allegro/herbert-base-cased\")\n",
        "\n",
        "def tensor_dataset(sentences, labels):\n",
        "\n",
        "  input_ids = []\n",
        "  attention_masks = []\n",
        "\n",
        "\n",
        "  for sent in sentences:\n",
        "   \n",
        "      encoded_dict = tokenizer.encode_plus(\n",
        "                          sent,                      # Sentence to encode.\n",
        "                          add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                          max_length = 64,           # Pad & truncate all sentences.\n",
        "                          pad_to_max_length = True,\n",
        "                          return_attention_mask = True,   # Construct attn. masks.\n",
        "                          return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                     )\n",
        "\n",
        "         \n",
        "      input_ids.append(encoded_dict['input_ids'])\n",
        "\n",
        "      attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "  # Convert the lists into tensors.\n",
        "  input_ids = torch.cat(input_ids, dim=0)\n",
        "  attention_masks = torch.cat(attention_masks, dim=0)\n",
        "  labels = torch.tensor(labels)\n",
        "  dataset = TensorDataset(input_ids, attention_masks, labels)\n",
        "  return dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-tjr2Dy9H6f"
      },
      "source": [
        "dataset = tensor_dataset(df['texts'], df['labels'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x97wnwSPopmF"
      },
      "source": [
        "val_size = int(0.2 * len(dataset))\n",
        "test_size = val_size\n",
        "train_size = (len(dataset) - val_size - test_size)\n",
        "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jn3xubO3a1vC",
        "outputId": "2d47f4a7-c617-4638-92ae-87aff5f19164"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-multilingual-uncased').to(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5F9Wr6Vpmla6"
      },
      "source": [
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EgVjdISpmvsC"
      },
      "source": [
        "epochs = 2\n",
        "train_dataloader = DataLoader(train_dataset)\n",
        "validation_dataloader = DataLoader(val_dataset)\n",
        "test_dataloader = DataLoader(test_dataset)\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, \n",
        "                                            num_training_steps = total_steps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjoeIhqlmz2B"
      },
      "source": [
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_fjSohzBnqas",
        "outputId": "349cb4e0-31ca-4e87-9142-9fd5b002c5ee"
      },
      "source": [
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "# We'll store a number of quantities such as training and validation loss, \n",
        "# validation accuracy, and timings.\n",
        "training_stats = []\n",
        "\n",
        "\n",
        "for epoch_i in range(0, epochs):\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    total_train_loss = 0\n",
        "\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "\n",
        "        model.zero_grad()        \n",
        "\n",
        "        result = model(b_input_ids, \n",
        "                       token_type_ids=None, \n",
        "                       attention_mask=b_input_mask, \n",
        "                       labels=b_labels,\n",
        "                       return_dict=True)\n",
        "\n",
        "        loss = result.loss\n",
        "        logits = result.logits\n",
        "\n",
        "        total_train_loss += loss.item()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
        "\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "\n",
        "      \n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "    nb_eval_steps = 0\n",
        "\n",
        "    for batch in validation_dataloader:\n",
        "        \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        with torch.no_grad():        \n",
        "\n",
        "            result = model(b_input_ids, \n",
        "                           token_type_ids=None, \n",
        "                           attention_mask=b_input_mask,\n",
        "                           labels=b_labels,\n",
        "                           return_dict=True)\n",
        "\n",
        "        loss = result.loss\n",
        "        logits = result.logits\n",
        "            \n",
        "        total_eval_loss += loss.item()\n",
        "\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
        "        \n",
        "\n",
        "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
        "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
        "\n",
        "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
        "    \n",
        "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "\n",
        "    training_stats.append(\n",
        "        {\n",
        "            'epoch': epoch_i + 1,\n",
        "            'Training Loss': avg_train_loss,\n",
        "            'Valid. Loss': avg_val_loss,\n",
        "            'Valid. Accur.': avg_val_accuracy,\n",
        "            \n",
        "        }\n",
        "    )\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 2 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.48\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.91\n",
            "  Validation Loss: 0.53\n",
            "\n",
            "======== Epoch 2 / 2 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.47\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.91\n",
            "  Validation Loss: 0.49\n",
            "\n",
            "Training complete!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hEzv4toaF6qR"
      },
      "source": [
        "## nn model in keras for task 1 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohH6LRyUc9ON"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np"
      ],
      "execution_count": 205,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmu1k1zrfLSD"
      },
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"allegro/herbert-base-cased\", \n",
        "                                          \n",
        "                                          add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                                          max_length = 64,           # Pad & truncate all sentences.\n",
        "                                          pad_to_max_length = True,\n",
        "                                          return_attention_mask = True,   # Construct attn. masks.\n",
        "                                          return_tensors = 'tf',\n",
        "                                          )\n"
      ],
      "execution_count": 206,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-L3QinAeEFR"
      },
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"allegro/herbert-base-cased\")\n",
        "\n",
        "def input_bert(sentences):\n",
        "\n",
        "  input_ids = []\n",
        "\n",
        "\n",
        "  for sent in sentences:\n",
        "   \n",
        "      encoded_dict = tokenizer.encode_plus(\n",
        "                          sent,                      # Sentence to encode.\n",
        "                          add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                          max_length = 64,           # Pad & truncate all sentences.\n",
        "                          pad_to_max_length = True,\n",
        "                          return_attention_mask = True,   # Construct attn. masks.\n",
        "                          return_tensors = 'tf',     # Return pytorch tensors.\n",
        "                     )\n",
        "      \n",
        "      i = np.array(encoded_dict['input_ids'])\n",
        "      print(i.shape)\n",
        "      i = i.flatten()\n",
        "      input_ids.append(i)\n",
        "\n",
        "  \n",
        "  return input_ids"
      ],
      "execution_count": 207,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O92nJ6hHIxz5"
      },
      "source": [
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "    num_words=None,\n",
        "    filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
        "    lower=True,\n",
        "    split=\" \",\n",
        "    char_level=False,\n",
        "    oov_token=None,\n",
        "    document_count=0,\n",
        ")\n",
        "\n",
        "tokenizer.fit_on_texts(df['texts'])\n",
        "# encoded = tokenizer.texts_to_sequences(df['texts'])"
      ],
      "execution_count": 208,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8X9sr7Y_iB2b"
      },
      "source": [
        "def input_keras(sentences):\n",
        "  input_ids = []\n",
        "  for lst in tokenizer.texts_to_sequences(sentences):\n",
        "    lst = np.array(lst)\n",
        "    lst = lst.flatten()\n",
        "\n",
        "    input_ids.append(lst)\n",
        "  input_ids = tf.keras.preprocessing.sequence.pad_sequences(input_ids)\n",
        "   \n",
        "  return input_ids "
      ],
      "execution_count": 209,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VT50JlDDc8EM",
        "outputId": "c29f2424-ec9f-4539-81c2-08e312eeba4c"
      },
      "source": [
        "# x = input_bert(df['texts'])  \n",
        "x = input_keras(df['texts'])\n",
        "y = df['labels']\n",
        "x_array = np.array(x)\n",
        "print(x_array.shape)\n",
        "y_array = np.array(y)\n",
        "# print(y_array.shape)"
      ],
      "execution_count": 214,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10041, 29)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zM58s3pgfCMG"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_train, x_rem, y_train, y_rem = train_test_split(x_array, y_array, train_size=0.8)\n",
        "x_valid, x_test, y_valid, y_test = train_test_split(x_rem, y_rem, test_size=0.5)"
      ],
      "execution_count": 215,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2c414ZHDugQV",
        "outputId": "6392688a-c23b-42ef-cb54-40b629fcc83c"
      },
      "source": [
        "train_true = list(y_train).count(1)\n",
        "train_false = list(y_train).count(0)\n",
        "train_count = train_true/train_false\n",
        "valid_true = list(y_valid).count(1)\n",
        "valid_false = list(y_valid).count(0)\n",
        "valid_count = valid_true/valid_false\n",
        "test_true = list(y_test).count(1)\n",
        "test_false = list(y_test).count(0)\n",
        "test_count = test_true/test_false\n",
        "print(train_count, ' ', valid_count, ' ', test_count)\n"
      ],
      "execution_count": 216,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.09353301565690947   0.09368191721132897   0.08414239482200647\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cZWnstlpzQI1",
        "outputId": "42a6f321-241c-4d96-92f2-274035886abe"
      },
      "source": [
        "gpus = tf.config.list_logical_devices('GPU')\n",
        "print(gpus)\n",
        "print(gpus[0].name)\n",
        "device = tf.device(gpus[0].name)"
      ],
      "execution_count": 217,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[LogicalDevice(name='/device:GPU:0', device_type='GPU')]\n",
            "/device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6PKWaydPHWwj"
      },
      "source": [
        "vocab_len = len(tokenizer.word_index)  # for keras tokenizer\n",
        "# vocab_len = len(tokenizer.get_vocab())     #for bert tokenizer\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    # encoder,\n",
        "    tf.keras.layers.Embedding(\n",
        "        input_dim=vocab_len,\n",
        "        output_dim=64,\n",
        "        # Use masking to handle the variable sequence lengths\n",
        "        mask_zero=True),\n",
        "    tf.keras.layers.LSTM(64),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])"
      ],
      "execution_count": 218,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n4j2b88NffUt",
        "outputId": "58eb4d4f-c711-4d62-f6d2-d10d41051ebf"
      },
      "source": [
        "opt = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
        "loss = tf.keras.losses.BinaryCrossentropy()\n",
        "# metrics = tf.keras.metrics.BinaryAccuracy()\n",
        "metrics = [tf.keras.metrics.FalseNegatives(), tf.keras.metrics.BinaryAccuracy()]\n",
        "\n",
        "model.compile(optimizer=opt, loss=loss, metrics=metrics)\n",
        "history = model.fit(x_train, y_train, epochs=3, validation_data=(x_valid, y_valid))"
      ],
      "execution_count": 219,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "251/251 [==============================] - 12s 36ms/step - loss: 0.2774 - false_negatives_14: 682.0000 - binary_accuracy: 0.9135 - val_loss: 0.2313 - val_false_negatives_14: 78.0000 - val_binary_accuracy: 0.9224\n",
            "Epoch 2/3\n",
            "251/251 [==============================] - 8s 31ms/step - loss: 0.0983 - false_negatives_14: 171.0000 - binary_accuracy: 0.9625 - val_loss: 0.3081 - val_false_negatives_14: 62.0000 - val_binary_accuracy: 0.9343\n",
            "Epoch 3/3\n",
            "251/251 [==============================] - 8s 31ms/step - loss: 0.0251 - false_negatives_14: 39.0000 - binary_accuracy: 0.9909 - val_loss: 0.3730 - val_false_negatives_14: 48.0000 - val_binary_accuracy: 0.9025\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aIxfovy4gEdf",
        "outputId": "dd1bf627-b77d-4100-ff62-c928e3f801cb"
      },
      "source": [
        "print(\"Evaluate on test data\")\n",
        "results = model.evaluate(x_test, y_test, batch_size=128)\n",
        "print(\"test loss, test acc:\", results)"
      ],
      "execution_count": 223,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluate on test data\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.3730 - false_negatives_14: 48.0000 - binary_accuracy: 0.9025\n",
            "test loss, test acc: [0.3729604482650757, 48.0, 0.902487576007843]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jkHiUMAuxO1J",
        "outputId": "471f099c-197b-4099-d7bb-1bb3506dcc09"
      },
      "source": [
        "predictions = model.predict(x_test)\n",
        "print(predictions[:10])"
      ],
      "execution_count": 224,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[2.4709408e-04]\n",
            " [7.2311186e-06]\n",
            " [1.9983359e-02]\n",
            " [3.3694605e-04]\n",
            " [2.8943452e-03]\n",
            " [3.1133395e-06]\n",
            " [6.9879908e-03]\n",
            " [3.9171995e-04]\n",
            " [9.4164526e-01]\n",
            " [2.0697504e-05]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ad__smSnug1z",
        "outputId": "3a8fa6e7-c21c-4587-8165-9f02742fb34d"
      },
      "source": [
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "\n",
        "predictions = [int(x) for x in predictions]\n",
        "stats = precision_recall_fscore_support(y_test, predictions, average='macro')\n",
        "print('precision ', stats[0])\n",
        "print('recall ', stats[1])\n",
        "print('fscore ', stats[2])"
      ],
      "execution_count": 225,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "precision  0.46119402985074626\n",
            "recall  0.5\n",
            "fscore  0.47981366459627334\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40fikXbP9gw3"
      },
      "source": [
        "## keras model for task 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWG7ppNN9pUm"
      },
      "source": [
        "df2 = create_dataset('/content/drive/MyDrive/CyberbullyingDetection/task2/training_set_clean_only_text.txt', '/content/drive/MyDrive/CyberbullyingDetection/task2/training_set_clean_only_tags.txt')"
      ],
      "execution_count": 227,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "moks5pB7_S--",
        "outputId": "c6a05d56-ecb3-4ab4-94d2-481b42bafeaf"
      },
      "source": [
        "df2.head()"
      ],
      "execution_count": 228,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>texts</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>dla mnie faworytem do tytuÅ‚u bÄ™dzie cracovia z...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>brawo ty daria kibic ma byÄ‡ na dobre i zÅ‚e</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>super polski premier skÅ‚ada kwiaty na grobac...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>musi innej drogi nie mamy</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>odrzut natychmiastowy kwaÅ›na mina mam problem</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               texts  labels\n",
              "0  dla mnie faworytem do tytuÅ‚u bÄ™dzie cracovia z...       0\n",
              "1         brawo ty daria kibic ma byÄ‡ na dobre i zÅ‚e       0\n",
              "2    super polski premier skÅ‚ada kwiaty na grobac...       0\n",
              "3                          musi innej drogi nie mamy       0\n",
              "4      odrzut natychmiastowy kwaÅ›na mina mam problem       0"
            ]
          },
          "metadata": {},
          "execution_count": 228
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfLzMVOBCagQ",
        "outputId": "d433b586-3c6e-4b0a-c9d3-454ab5c77751",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install keras.utils"
      ],
      "execution_count": 231,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting keras.utils\n",
            "  Downloading keras-utils-1.0.13.tar.gz (2.4 kB)\n",
            "Requirement already satisfied: Keras>=2.1.5 in /usr/local/lib/python3.7/dist-packages (from keras.utils) (2.6.0)\n",
            "Building wheels for collected packages: keras.utils\n",
            "  Building wheel for keras.utils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras.utils: filename=keras_utils-1.0.13-py3-none-any.whl size=2656 sha256=dc26ccc0b3c0dce6809055cd53ab9527c8fa6c2ac3b74eeb234b04b47406affb\n",
            "  Stored in directory: /root/.cache/pip/wheels/d0/dd/3b/493952a5240d486a83805d65360dedadbadeae71d25e2c877f\n",
            "Successfully built keras.utils\n",
            "Installing collected packages: keras.utils\n",
            "Successfully installed keras.utils-1.0.13\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bl9gC1AAA7BF",
        "outputId": "413f6c9f-0e08-4de9-ab3e-ba2bdabf0a6a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "# define example\n",
        "data = df2['labels']\n",
        "data = array(data)\n",
        "# print(data)\n",
        "# one hot encode\n",
        "encoded = to_categorical(data)\n",
        "print(encoded)\n"
      ],
      "execution_count": 234,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " ...\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bqTmgib7_zYe",
        "outputId": "c8bb16bb-de26-4355-af40-c40ee4aa69a1"
      },
      "source": [
        "tokenizer.fit_on_texts(df2['texts'])\n",
        "x = input_keras(df2['texts'])\n",
        "\n",
        "x_array = np.array(x)\n",
        "print(x_array.shape)\n",
        "y_array = np.array(encoded)"
      ],
      "execution_count": 237,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10041, 29)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1pAUC5QXC_yF"
      },
      "source": [
        "x_train, x_rem, y_train, y_rem = train_test_split(x_array, y_array, train_size=0.8)\n",
        "x_valid, x_test, y_valid, y_test = train_test_split(x_rem, y_rem, test_size=0.5)"
      ],
      "execution_count": 239,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m0aFAx7KDJ6B"
      },
      "source": [
        "vocab_len = len(tokenizer.word_index)  # for keras tokenizer\n",
        "# vocab_len = len(tokenizer.get_vocab())     #for bert tokenizer\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    # encoder,\n",
        "    tf.keras.layers.Embedding(\n",
        "        input_dim=vocab_len,\n",
        "        output_dim=64,\n",
        "        # Use masking to handle the variable sequence lengths\n",
        "        mask_zero=True),\n",
        "    tf.keras.layers.LSTM(64),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(16),\n",
        "\n",
        "    tf.keras.layers.Dense(3, activation='sigmoid')\n",
        "])"
      ],
      "execution_count": 255,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "loFdb8xvDppZ",
        "outputId": "a6b5cd2a-8464-424d-c547-d0ac84752a70",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "opt = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
        "loss = tf.keras.losses.CategoricalCrossentropy()\n",
        "metrics = tf.keras.metrics.CategoricalAccuracy()\n",
        "\n",
        "model.compile(optimizer=opt, loss=loss, metrics=metrics)\n",
        "history = model.fit(x_train, y_train, epochs=5, validation_data=(x_valid, y_valid))"
      ],
      "execution_count": 260,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "251/251 [==============================] - 11s 35ms/step - loss: 0.0116 - categorical_accuracy: 0.9973 - val_loss: 0.9657 - val_categorical_accuracy: 0.8855\n",
            "Epoch 2/5\n",
            "251/251 [==============================] - 8s 31ms/step - loss: 0.0056 - categorical_accuracy: 0.9981 - val_loss: 0.8018 - val_categorical_accuracy: 0.9143\n",
            "Epoch 3/5\n",
            "251/251 [==============================] - 8s 32ms/step - loss: 0.0022 - categorical_accuracy: 0.9993 - val_loss: 1.1138 - val_categorical_accuracy: 0.8914\n",
            "Epoch 4/5\n",
            "251/251 [==============================] - 8s 31ms/step - loss: 0.0028 - categorical_accuracy: 0.9991 - val_loss: 1.3205 - val_categorical_accuracy: 0.8795\n",
            "Epoch 5/5\n",
            "251/251 [==============================] - 8s 31ms/step - loss: 0.0052 - categorical_accuracy: 0.9989 - val_loss: 2.1665 - val_categorical_accuracy: 0.8446\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kyZ4FgbxD9wW",
        "outputId": "d0d74dc8-8c14-44e8-dd6a-569fee46bb68",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(\"Evaluate on test data\")\n",
        "results = model.evaluate(x_test, y_test, batch_size=128)\n",
        "print(\"test loss, test acc:\", results)"
      ],
      "execution_count": 261,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluate on test data\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 1.9136 - categorical_accuracy: 0.8716\n",
            "test loss, test acc: [1.9136103391647339, 0.8716418147087097]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gEKiDKi2Qoot"
      },
      "source": [
        "predictions = model.predict(x_test)\n",
        "predictions = [[int(x) for x in pred] for pred in predictions]"
      ],
      "execution_count": 264,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NOeCYHhpLlBs",
        "outputId": "bfaa984d-016b-4466-d180-6587f372c2f3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "stats = precision_recall_fscore_support(y_test, predictions, average='macro')\n",
        "\n",
        "print('macro')\n",
        "print('fscore ', stats[2])\n",
        "\n",
        "stats = precision_recall_fscore_support(y_test, predictions, average='micro')\n",
        "\n",
        "print('micro')\n",
        "print('fscore ', stats[2])"
      ],
      "execution_count": 266,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "macro\n",
            "fscore  0.4879788339176731\n",
            "micro\n",
            "fscore  0.729064039408867\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}